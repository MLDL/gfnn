{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = [\"wisconsin\", \"cora\", \"citeseer\", \"pubmed\", \n",
    "              \"cornell\", \"texas\", \"chameleon\", \"squirrel\", \n",
    "              \"film\", \"photo\", \"computer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loaders(X, y, train_idx, val_idx, test_idx, batch_size=32):\n",
    "    train = torch.utils.data.TensorDataset(X[train_idx], y[train_idx])\n",
    "    val = torch.utils.data.TensorDataset(X[val_idx], y[val_idx])\n",
    "    test = torch.utils.data.TensorDataset(X[test_idx], y[test_idx])\n",
    "    trainset = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valset = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=False)\n",
    "    testset = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    return trainset, valset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    def __init__(self, idim, nclass):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(idim, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 100)\n",
    "        self.fc4 = nn.Linear(100, nclass)\n",
    "        self.bn = nn.BatchNorm1d(100)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "class SMLP(Module):\n",
    "    def __init__(self, idim, nclass):\n",
    "        super(SMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(idim, 100)\n",
    "        self.fc2 = nn.Linear(100, nclass)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mlp(model_class, dname, split=\"0.6_0.2_0.2\", bs=16, s=None):\n",
    "    gnx, normed_adj, X, y, idx_train, idx_val, idx_test = load_data(dname, [''], split=split, random_state=s)\n",
    "    trainloader, valloader, testloader = data_loaders(X, y, idx_train, idx_val, idx_test)\n",
    "    batch_size = bs\n",
    "    model = MLP(int(X.size(1)), int(y.max()+1))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    obj = nn.CrossEntropyLoss()\n",
    "    epochs = 500\n",
    "    best_val = 0.0\n",
    "    stats = {\n",
    "        \"tr_loss\": [],\n",
    "        \"tr_acc\": [],\n",
    "        \"tr_loss_e\": [],\n",
    "        \"tr_acc_e\": [],\n",
    "        \"va_loss\": [],\n",
    "        \"va_acc\": [],\n",
    "        \"va_loss_e\": [],\n",
    "        \"va_acc_e\": [],\n",
    "        \"te_acc\": [],\n",
    "        \"te_loss\": [],\n",
    "        \"best_epochs\": []\n",
    "    }\n",
    "    # Train\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train() \n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        ndata = len(trainloader.dataset)\n",
    "        for i, (X,y) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X)\n",
    "            pred = out.data.max(1)[1]\n",
    "            loss = obj(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            stats[\"tr_loss\"].append(loss.item())\n",
    "            stats[\"tr_acc\"].append(np.sum(pred.cpu().numpy() == y.data.cpu().numpy()) / len(y))\n",
    "            train_loss += stats[\"tr_loss\"][-1]\n",
    "            train_acc += np.sum(pred.cpu().numpy() == y.data.cpu().numpy())\n",
    "            \n",
    "            # Eval per batch\n",
    "            model.eval() \n",
    "            val_loss, val_acc = 0.0, 0.0\n",
    "            ndata = len(valloader.dataset)\n",
    "            for i, (X,y) in enumerate(valloader):\n",
    "                out = model(X)\n",
    "                pred = out.data.max(1)[1]\n",
    "                loss = obj(out, y)\n",
    "                val_loss += loss.item()\n",
    "                val_acc += np.sum(pred.cpu().numpy() == y.data.cpu().numpy())\n",
    "        \n",
    "            stats[\"va_loss\"].append(val_loss / ndata)\n",
    "            stats[\"va_acc\"].append(val_acc / ndata * 100)\n",
    "    \n",
    "        stats[\"tr_loss_e\"].append(train_loss / ndata)\n",
    "        stats[\"tr_acc_e\"].append(train_acc / ndata * 100)\n",
    "    \n",
    "        # Eval per epoch\n",
    "        model.eval() \n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        ndata = len(valloader.dataset)\n",
    "        for i, (X,y) in enumerate(valloader):\n",
    "            out = model(X)\n",
    "            pred = out.data.max(1)[1]\n",
    "            loss = obj(out, y)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += np.sum(pred.cpu().numpy() == y.data.cpu().numpy())\n",
    "\n",
    "        stats[\"va_loss_e\"].append(val_loss / ndata)\n",
    "        stats[\"va_acc_e\"].append(val_acc / ndata * 100)\n",
    "        \n",
    "        # Check if best val\n",
    "        if (stats[\"va_acc_e\"][-1] > best_val):\n",
    "            best_val = stats[\"va_acc_e\"][-1]\n",
    "            # Test\n",
    "            model.eval() \n",
    "            te_loss, te_acc = 0.0, 0.0\n",
    "            ndata = len(testloader.dataset)\n",
    "            for i, (X,y) in enumerate(testloader):\n",
    "                out = model(X)\n",
    "                pred = out.data.max(1)[1]\n",
    "                loss = obj(out, y)\n",
    "                te_loss += loss.item()\n",
    "                te_acc += np.sum(pred.cpu().numpy() == y.data.cpu().numpy())\n",
    "            \n",
    "            stats['te_loss'].append(te_loss / ndata)\n",
    "            stats['te_acc'].append(te_acc / ndata * 100)\n",
    "            stats['best_epochs'].append(epoch)\n",
    "            \n",
    "    print(stats['te_acc'][-1])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.27450980392157\n",
      "88.23529411764706\n",
      "88.23529411764706\n",
      "88.23529411764706\n",
      "80.3921568627451\n",
      "86.27450980392157\n",
      "80.3921568627451\n",
      "88.23529411764706\n",
      "82.35294117647058\n",
      "84.31372549019608\n"
     ]
    }
   ],
   "source": [
    "stat_wis = [evaluate_mlp(MLP, \"wisconsin\", s=seeds[i]) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.29411764705883"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([stat_wis[i]['te_acc'][-1] for i in range(10)]) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.306911291029383"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([stat_wis[i]['te_acc'][-1] for i in range(10)]) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.3921568627451\n",
      "82.35294117647058\n",
      "86.27450980392157\n",
      "90.19607843137256\n",
      "80.3921568627451\n",
      "80.3921568627451\n",
      "82.35294117647058\n",
      "88.23529411764706\n",
      "84.31372549019608\n",
      "86.27450980392157\n"
     ]
    }
   ],
   "source": [
    "stat_wis_smlp = [evaluate_mlp(SMLP, \"wisconsin\", s=seeds[i]) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.13725490196079"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([stat_wis_smlp[i]['te_acc'][-1] for i in range(10)]) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42236586722623565"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([stat_wis_smlp[i]['te_acc'][-1] for i in range(10)]) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.84870848708486\n",
      "70.47970479704797\n",
      "68.26568265682657\n",
      "69.74169741697416\n",
      "68.26568265682657\n",
      "68.81918819188192\n"
     ]
    }
   ],
   "source": [
    "all_stats = dict({\n",
    "    \"wisconsin\": stat_wis\n",
    "})\n",
    "for dn in data_names[1:]:\n",
    "    all_stats[dn] = [evaluate_mlp(MLP, dn, s=seeds[i]) for i in range(10)]\n",
    "    print(dn)\n",
    "    print(np.average([all_stats[dn][i]['te_acc'][-1] for i in range(10)]))\n",
    "    print(np.std([all_stats[dn][i]['te_acc'][-1] for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
